# Parameters can be overwritten on the command line
# for example: cntk configFile=myConfigFile RootDir=../.. 
# For running from Visual Studio add
# currentDirectory=$(SolutionDir)/<path to corresponding data folder> 
RootDir = "."

ConfigDir = "$RootDir$"
DataDir   = "../../data"
OutputDir = "$RootDir$/Output"
ModelDir  = "$OutputDir$/Models"

# deviceId=-1 for CPU, >=0 for GPU devices, "auto" chooses the best GPU, or CPU if no usable GPU is available
epochSize=4096
deviceId=0 
minibatch=256
maxEpochs = 2 

command = train

precision  = "float"
traceLevel = 1
modelPath  = "$ModelDir$/rnn.dnn"

# uncomment the following line to write logs to a file
#stderr=$OutputDir$/rnnOutput

#numCPUThreads = 1

confVocabSize = 10000

trainFile = "ptb.train.txt"

#Uncomment the following if you get an out of memory error
#shareNodeValueMatrices = true

#######################################
#  TRAINING CONFIG                    #
#######################################

train = [
    action = "train"
    traceLevel = 1
    epochSize = 0               # (for quick tests, this can be overridden with something small)

    BrainScriptNetworkBuilder = [
        inputDim = $confVocabSize$
        labelDim = $confVocabSize$
        embedDim = 256
        hiddenDim = (256:256)
        
        model(x) = [
            E = ParameterTensor (embedDim:inputDim)
            e = E * x
            h = BS.RNNs.RecurrentLSTMPStack (hiddenDim, cellDims=hiddenDim, e, inputDim=embedDim)[1].h
            W = ParameterTensor (labelDim:hiddenDim[1]) 
            b = ParameterTensor (labelDim,init='fixedValue',value=0);
            z = W * h + b
        ].z

#        model_v17 = Sequential (
#            EmbeddingLayer (embedDim) :
#            RecurrentLSTMLayerStack (hiddenDim, allowOptimizedEngine = false) : 
#            DenseLayer (labelDim)
#        )

        words  = if deviceId >= 0 then SparseInput (inputDim) else Input (inputDim)
        labels = Input (inputDim)

        z = model(words)

        # for sparse labels 
        # crossEntropy = ReduceLogSum(z, axis=1) - TransposeTimes(labels,z)
        # for dense labels
        crossEntropy = CrossEntropyWithSoftmax(labels, z)
        # TODO use sampled softmax

        featureNodes    = (words)
        labelNodes      = (labels)
        criterionNodes  = (crossEntropy)
        evaluationNodes = (crossEntropy)
        outputNodes     = (z)
    ]
    
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/$trainFile$"
        # fast but a memory hog
        randomize = false
        # slower but memory friendly
        #randomize = true 
        #randomizationWindow = 30000
        traceLevel = 0
        input = [
            words  = [ alias = "S0"; dim = $confVocabSize$; format = "sparse" ]
            labels = [ alias = "S1"; dim = $confVocabSize$; format = "sparse" ]
        ]
    ]
   
    SGD = [
        #minibatchSize = 128:256:512
        epochSize=$epochSize$
        minibatchSize = $minibatch$ 
        learningRatesPerSample = 0.0001
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = $maxEpochs$ 
        numMBsToShowResult = 1
        gradUpdateType = "none"
        loadBestModel = true

        dropoutRate = 0.0

        # settings for Auto Adjust Learning Rate
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
]

